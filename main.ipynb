{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4768f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import  accuracy_score , f1_score\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8bb4ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(trainfile='MINIST_train.csv', validationfile='MNIST_validation.csv'):\n",
    "\n",
    "    # Load CSVs\n",
    "    dftrain = pd.read_csv(trainfile)\n",
    "    dfval = pd.read_csv(validationfile)\n",
    "\n",
    "    # All pixel columns (784 features)\n",
    "    featurecols = [col for col in dftrain.columns if col not in ['label', 'even']]\n",
    "\n",
    "    targetcol = 'label'   # multiclass (0–9)\n",
    "\n",
    "    # Extract X and y\n",
    "    Xtrain = dftrain[featurecols].values.astype(float)\n",
    "    ytrain = dftrain[targetcol].values.astype(int)\n",
    "\n",
    "    Xval = dfval[featurecols].values.astype(float)\n",
    "    yval = dfval[targetcol].values.astype(int)\n",
    "\n",
    "    return Xtrain, ytrain, Xval, yval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd854cf",
   "metadata": {},
   "source": [
    "SVM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "59087ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_p=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_p = lambda_p\n",
    "        self.n_iters = n_iters\n",
    "        self.W = None     # shape: (K, D)\n",
    "        self.b = None     # shape: (K,)\n",
    "\n",
    "    def _binary_svm_update(self, X, y, w, b):\n",
    "        \"\"\"\n",
    "        Performs SGD updates for ONE binary classifier.\n",
    "        X: (N, D)\n",
    "        y: (N,) labels in {-1, +1}\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, w) - b) >= 1\n",
    "\n",
    "                if condition:\n",
    "                    # only regularization affects gradient\n",
    "                    w -= self.lr * (self.lambda_p * w)\n",
    "                else:\n",
    "                    # hinge loss + regularization\n",
    "                    w -= self.lr * (self.lambda_p * w - y[idx] * x_i)\n",
    "                    b -= self.lr * y[idx]\n",
    "\n",
    "        return w, b\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train K binary classifiers (one-vs-rest) using the above SGD procedure.\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        classes = np.unique(y)\n",
    "        K = len(classes)\n",
    "\n",
    "        # initialize parameters\n",
    "        self.W = np.zeros((K, D))\n",
    "        self.b = np.zeros(K)\n",
    "\n",
    "        # Train each class separately\n",
    "        for k in classes:\n",
    "            print(f\"Training class {k} vs rest...\")\n",
    "\n",
    "            # Convert labels to +1 (class k) and -1 (all others)\n",
    "            y_binary = np.where(y == k, 1, -1)\n",
    "\n",
    "            # Extract w_k, b_k\n",
    "            w_k = self.W[k].copy()\n",
    "            b_k = self.b[k].copy()\n",
    "\n",
    "            # Train binary classifier\n",
    "            w_k, b_k = self._binary_svm_update(X, y_binary, w_k, b_k)\n",
    "\n",
    "            # Store results\n",
    "            self.W[k] = w_k\n",
    "            self.b[k] = b_k\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Compute scores for each class\n",
    "        scores = X.dot(self.W.T) - self.b\n",
    "        return np.argmax(scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad83b38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training class 0 vs rest...\n",
      "Training class 1 vs rest...\n",
      "Training class 2 vs rest...\n",
      "Training class 3 vs rest...\n",
      "Training class 4 vs rest...\n",
      "Training class 5 vs rest...\n",
      "Training class 6 vs rest...\n",
      "Training class 7 vs rest...\n",
      "Training class 8 vs rest...\n",
      "Training class 9 vs rest...\n",
      "Validation Accuracy: 0.8931572629051621\n",
      "Macro F1-score: 0.8906909496517933\n",
      "Training Time: 6.67 seconds (0.11 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "Xtrain, ytrain, Xval, yval =  read_data('MNIST_train.csv', 'MNIST_validation.csv')\n",
    "# Normalize\n",
    "Xtrain = Xtrain / 255.0\n",
    "Xval   = Xval / 255.0\n",
    "\n",
    "svm = MulticlassSVM(\n",
    "    learning_rate=0.0015,\n",
    "    lambda_p=0.01,\n",
    "    n_iters=5        # SGD loops INSIDE classes → keep smaller\n",
    ")\n",
    "\n",
    "start_time = time()\n",
    "svm.fit(Xtrain, ytrain)\n",
    "\n",
    "end_time = time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "preds = svm.predict(Xval)\n",
    "acc = np.mean(preds == yval)\n",
    "\n",
    "f1 = f1_score(yval, preds, average='macro')\n",
    "\n",
    "print(\"Validation Accuracy:\", acc)\n",
    "print(\"Macro F1-score:\", f1)\n",
    "print(\"Training Time: {:.2f} seconds ({:.2f} minutes)\".format(\n",
    "    training_time, training_time / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530f396",
   "metadata": {},
   "source": [
    "Logistic Regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "81ae1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionMultiClass:\n",
    "    def __init__(self, lr=0.2, n_epochs=40, mini_batch_size=256):\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.classes = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -50, 50)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def standardize(self, X, train=True):\n",
    "        if train:\n",
    "            self.mean = X.mean(axis=0)\n",
    "            self.std = X.std(axis=0) + 1e-8\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # STANDARDIZE ONCE\n",
    "        X = self.standardize(X, train=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Unique classes 0–9\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "\n",
    "        # Parameters for each classifier (one-vs-rest)\n",
    "        self.weights = np.zeros((n_classes, n_features))\n",
    "        self.bias = np.zeros(n_classes)\n",
    "\n",
    "        # Train one classifier per digit\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            print(f\"Training class {c} vs rest\")\n",
    "\n",
    "            y_binary = (y == c).astype(int)\n",
    "\n",
    "            w = np.zeros(n_features)\n",
    "            b = 0\n",
    "\n",
    "            for epoch in range(self.n_epochs):\n",
    "\n",
    "                # Shuffle indices\n",
    "                indices = np.arange(n_samples)\n",
    "                np.random.shuffle(indices)\n",
    "                X_shuffled = X[indices]\n",
    "                y_shuffled = y_binary[indices]\n",
    "\n",
    "                # Mini-batch SGD\n",
    "                for i in range(0, n_samples, self.mini_batch_size):\n",
    "                    X_batch = X_shuffled[i:i+self.mini_batch_size]\n",
    "                    y_batch = y_shuffled[i:i+self.mini_batch_size]\n",
    "\n",
    "                    z = np.dot(X_batch, w) + b\n",
    "                    y_pred = self.sigmoid(z)\n",
    "\n",
    "                    dw = np.dot(X_batch.T, (y_pred - y_batch)) / len(y_batch)\n",
    "                    db = np.sum(y_pred - y_batch) / len(y_batch)\n",
    "\n",
    "                    w -= self.lr * dw\n",
    "                    b -= self.lr * db\n",
    "\n",
    "            self.weights[idx] = w\n",
    "            self.bias[idx] = b\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.standardize(X, train=False)\n",
    "        logits = np.dot(X, self.weights.T) + self.bias\n",
    "        probs = self.sigmoid(logits)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "160abea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training class 0 vs rest\n",
      "Training class 1 vs rest\n",
      "Training class 2 vs rest\n",
      "Training class 3 vs rest\n",
      "Training class 4 vs rest\n",
      "Training class 5 vs rest\n",
      "Training class 6 vs rest\n",
      "Training class 7 vs rest\n",
      "Training class 8 vs rest\n",
      "Training class 9 vs rest\n",
      "Validation Accuracy = 0.8883553421368547\n",
      "Macro F1-score = 0.8868149537588608\n",
      "Training Time: 54.21 seconds (0.90 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "Xtrain, ytrain, Xval, yval = read_data('MNIST_train.csv', 'MNIST_validation.csv')\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegressionMultiClass(lr=0.20,n_epochs=40,mini_batch_size=256)\n",
    "\n",
    "start_time = time()\n",
    "# Train\n",
    "model.fit(Xtrain, ytrain)\n",
    "\n",
    "end_time = time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Predict\n",
    "ypred = model.predict(Xval)\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "\n",
    "acc = accuracy_score(yval, ypred)\n",
    "f1 = f1_score(yval, ypred, average='macro')\n",
    "\n",
    "print(\"Validation Accuracy =\", acc)\n",
    "print(\"Macro F1-score =\", f1)\n",
    "print(\"Training Time: {:.2f} seconds ({:.2f} minutes)\".format(\n",
    "    training_time, training_time / 60)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42cd22",
   "metadata": {},
   "source": [
    "KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43cb2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=5, batch_size=500):\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _normalize(self, X):\n",
    "        # Scale to 0–1\n",
    "        X = X.astype(np.float32) / 255.0\n",
    "        # L2 normalize each sample (very important for cosine distance)\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-6\n",
    "        return X / norms\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = self._normalize(X)\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self._normalize(X)\n",
    "        n_test = X.shape[0]\n",
    "        predictions = np.zeros(n_test, dtype=int)\n",
    "\n",
    "        for i in range(0, n_test, self.batch_size):\n",
    "            X_batch = X[i:i+self.batch_size]\n",
    "\n",
    "            # Cosine similarity = dot(x, y)\n",
    "            sim = np.dot(X_batch, self.X_train.T)\n",
    "\n",
    "            # Convert similarity to distance\n",
    "            dists = 1 - sim   # smaller = closer\n",
    "\n",
    "            # Find top-k nearest neighbors\n",
    "            k_idx = np.argpartition(dists, self.k, axis=1)[:, :self.k]\n",
    "\n",
    "            k_labels = self.y_train[k_idx]\n",
    "            k_dists = dists[np.arange(dists.shape[0])[:, None], k_idx]\n",
    "\n",
    "            # Weight = 1 / distance (closer neighbor gets more power)\n",
    "            weights = 1 / (k_dists + 1e-6)\n",
    "\n",
    "            # Weighted vote\n",
    "            batch_pred = [\n",
    "                np.bincount(k_labels[row], weights=weights[row]).argmax()\n",
    "                for row in range(k_labels.shape[0])\n",
    "            ]\n",
    "\n",
    "            predictions[i:i+self.batch_size] = batch_pred\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "010ca553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9575830332132853\n",
      "Macro F1-score = 0.9570664232264271\n",
      "Training Time: 0.08 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "Xtrain, ytrain, Xval, yval = read_data('MNIST_train.csv', 'MNIST_validation.csv')\n",
    "\n",
    "# Train (lazy)\n",
    "model = KNN(k=3)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "model.fit(Xtrain, ytrain)\n",
    "\n",
    "end_time = time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Predict\n",
    "ypred = model.predict(Xval)\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "acc = accuracy_score(yval, ypred)\n",
    "f1 = f1_score(yval, ypred, average='macro')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy =\", acc)\n",
    "print(\"Macro F1-score =\", f1)\n",
    "print(\"Training Time: {:.2f} seconds\".format(training_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
