{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b82ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4e346",
   "metadata": {},
   "source": [
    "Logistic Regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ffa83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionMultiClass:\n",
    "    def __init__(self, lr=0.2, n_epochs=40, mini_batch_size=256):\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.classes = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -50, 50)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def standardize(self, X, train=True):\n",
    "        if train:\n",
    "            self.mean = X.mean(axis=0)\n",
    "            self.std = X.std(axis=0) + 1e-8\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # STANDARDIZE ONCE\n",
    "        X = self.standardize(X, train=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Unique classes 0–9\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "\n",
    "        # Parameters for each classifier (one-vs-rest)\n",
    "        self.weights = np.zeros((n_classes, n_features))\n",
    "        self.bias = np.zeros(n_classes)\n",
    "\n",
    "        # Train one classifier per digit\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            print(f\"Training class {c} vs rest\")\n",
    "\n",
    "            y_binary = (y == c).astype(int)\n",
    "\n",
    "            w = np.zeros(n_features)\n",
    "            b = 0\n",
    "\n",
    "            for epoch in range(self.n_epochs):\n",
    "\n",
    "                # Shuffle indices\n",
    "                indices = np.arange(n_samples)\n",
    "                np.random.shuffle(indices)\n",
    "                X_shuffled = X[indices]\n",
    "                y_shuffled = y_binary[indices]\n",
    "\n",
    "                # Mini-batch SGD\n",
    "                for i in range(0, n_samples, self.mini_batch_size):\n",
    "                    X_batch = X_shuffled[i:i+self.mini_batch_size]\n",
    "                    y_batch = y_shuffled[i:i+self.mini_batch_size]\n",
    "\n",
    "                    z = np.dot(X_batch, w) + b\n",
    "                    y_pred = self.sigmoid(z)\n",
    "\n",
    "                    dw = np.dot(X_batch.T, (y_pred - y_batch)) / len(y_batch)\n",
    "                    db = np.sum(y_pred - y_batch) / len(y_batch)\n",
    "\n",
    "                    w -= self.lr * dw\n",
    "                    b -= self.lr * db\n",
    "\n",
    "            self.weights[idx] = w\n",
    "            self.bias[idx] = b\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.standardize(X, train=False)\n",
    "        logits = np.dot(X, self.weights.T) + self.bias\n",
    "        probs = self.sigmoid(logits)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83759cc7",
   "metadata": {},
   "source": [
    "SVM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6b73b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_p=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_p = lambda_p\n",
    "        self.n_iters = n_iters\n",
    "        self.W = None     # shape: (K, D)\n",
    "        self.b = None     # shape: (K,)\n",
    "\n",
    "    def _binary_svm_update(self, X, y, w, b):\n",
    "        \"\"\"\n",
    "        Performs SGD updates for ONE binary classifier.\n",
    "        X: (N, D)\n",
    "        y: (N,) labels in {-1, +1}\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, w) - b) >= 1\n",
    "\n",
    "                if condition:\n",
    "                    # only regularization affects gradient\n",
    "                    w -= self.lr * (self.lambda_p * w)\n",
    "                else:\n",
    "                    # hinge loss + regularization\n",
    "                    w -= self.lr * (self.lambda_p * w - y[idx] * x_i)\n",
    "                    b -= self.lr * y[idx]\n",
    "\n",
    "        return w, b\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train K binary classifiers (one-vs-rest) using the above SGD procedure.\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        classes = np.unique(y)\n",
    "        K = len(classes)\n",
    "\n",
    "        # initialize parameters\n",
    "        self.W = np.zeros((K, D))\n",
    "        self.b = np.zeros(K)\n",
    "\n",
    "        # Train each class separately\n",
    "        for k in classes:\n",
    "            print(f\"Training class {k} vs rest...\")\n",
    "\n",
    "            # Convert labels to +1 (class k) and -1 (all others)\n",
    "            y_binary = np.where(y == k, 1, -1)\n",
    "\n",
    "            # Extract w_k, b_k\n",
    "            w_k = self.W[k].copy()\n",
    "            b_k = self.b[k].copy()\n",
    "\n",
    "            # Train binary classifier\n",
    "            w_k, b_k = self._binary_svm_update(X, y_binary, w_k, b_k)\n",
    "\n",
    "            # Store results\n",
    "            self.W[k] = w_k\n",
    "            self.b[k] = b_k\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Compute scores for each class\n",
    "        scores = X.dot(self.W.T) - self.b\n",
    "        return np.argmax(scores, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7d3b5",
   "metadata": {},
   "source": [
    "XGBoost algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91c86bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decision Stump for XGBoost\n",
    "class DecisionStumpXGB:\n",
    "\n",
    "    def __init__(self, depth=1, max_depth=1, reg_lambda=1.0, gamma=0.0, subsample_features=1.0, n_thresholds=10):\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.value = 0.0\n",
    "        self.left_stump = None\n",
    "        self.right_stump = None\n",
    "        self.is_leaf = False\n",
    "        self.is_valid = False\n",
    "\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.gamma = gamma\n",
    "        self.subsample_features = subsample_features\n",
    "        self.n_thresholds = n_thresholds\n",
    "\n",
    "    def leaf_value(self, grad, hess):\n",
    "        return -np.sum(grad) / (np.sum(hess) + self.reg_lambda)\n",
    "\n",
    "    def fit(self, X, grad, hess):\n",
    "        m, n = X.shape\n",
    "        assert grad.shape[0] == m\n",
    "\n",
    "        if self.depth >= self.max_depth:\n",
    "            self.is_leaf = True\n",
    "            self.is_valid = True\n",
    "            self.value = self.leaf_value(grad, hess)\n",
    "            return\n",
    "\n",
    "        best_gain = -float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_masks = None\n",
    "        n_sub = max(1, int(self.subsample_features * n))\n",
    "        feature_indices = np.random.choice(n, n_sub, replace=False)\n",
    "\n",
    "        for feature_index in feature_indices:\n",
    "            col = X[:, feature_index]\n",
    "            if np.isnan(col).any():\n",
    "                continue\n",
    "\n",
    "            quantiles = np.linspace(0, 1, num=self.n_thresholds)\n",
    "            thresholds = np.unique(np.quantile(col, quantiles))\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                left_mask = col <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                G_left, H_left = np.sum(grad[left_mask]), np.sum(hess[left_mask])\n",
    "                G_right, H_right = np.sum(grad[right_mask]), np.sum(hess[right_mask])\n",
    "\n",
    "                gain = 0.5 * (\n",
    "                    (G_left**2 / (H_left + self.reg_lambda)) + \n",
    "                    (G_right**2 / (H_right + self.reg_lambda)) - \n",
    "                    ((G_left + G_right)**2 / (H_left + H_right + self.reg_lambda))\n",
    "                ) - self.gamma\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_index\n",
    "                    best_threshold = threshold\n",
    "                    best_masks = (left_mask, right_mask)\n",
    "\n",
    "        if best_feature is None or best_gain <= 0:\n",
    "            self.is_leaf = True\n",
    "            self.is_valid = True\n",
    "            self.value = self.leaf_value(grad, hess)\n",
    "            return\n",
    "\n",
    "        self.feature_index = best_feature\n",
    "        self.threshold = best_threshold\n",
    "        self.is_valid = True\n",
    "        left_mask, right_mask = best_masks\n",
    "\n",
    "        self.left_stump = DecisionStumpXGB(\n",
    "            depth=self.depth + 1, max_depth=self.max_depth,\n",
    "            reg_lambda=self.reg_lambda, gamma=self.gamma,\n",
    "            subsample_features=self.subsample_features, n_thresholds=self.n_thresholds\n",
    "        )\n",
    "\n",
    "        self.right_stump = DecisionStumpXGB(\n",
    "            depth=self.depth + 1, max_depth=self.max_depth,\n",
    "            reg_lambda=self.reg_lambda, gamma=self.gamma,\n",
    "            subsample_features=self.subsample_features, n_thresholds=self.n_thresholds\n",
    "        )\n",
    "\n",
    "        self.left_stump.fit(X[left_mask], grad[left_mask], hess[left_mask])\n",
    "        self.right_stump.fit(X[right_mask], grad[right_mask], hess[right_mask])\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.is_valid:\n",
    "            return np.zeros(X.shape[0], dtype=float)\n",
    "\n",
    "        if self.is_leaf or self.feature_index is None:\n",
    "            return np.full(X.shape[0], self.value, dtype=float)\n",
    "\n",
    "        feature = X[:, self.feature_index]\n",
    "        left_mask = feature <= self.threshold\n",
    "        right_mask = ~left_mask\n",
    "        preds = np.zeros(X.shape[0], dtype=float)\n",
    "\n",
    "        if self.left_stump:\n",
    "            preds[left_mask] = self.left_stump.predict(X[left_mask])\n",
    "        else:\n",
    "            preds[left_mask] = self.value\n",
    "\n",
    "        if self.right_stump:\n",
    "            preds[right_mask] = self.right_stump.predict(X[right_mask])\n",
    "        else:\n",
    "            preds[right_mask] = self.value\n",
    "\n",
    "        return preds\n",
    "\n",
    "# Binary XGBoost Classifier\n",
    "class XGBoostClassifier:\n",
    "\n",
    "    def __init__(self, n_estimators=40, learning_rate=0.1, max_depth=3, reg_lambda=1.0, gamma=0.1,\n",
    "                 subsample_features=0.3, n_thresholds=10, proba_threshold=0.5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.gamma = gamma\n",
    "        self.subsample_features = subsample_features\n",
    "        self.n_thresholds = n_thresholds\n",
    "        self.proba_threshold = proba_threshold\n",
    "        self.trees = []\n",
    "        self.init_pred = 0.0\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        x = np.clip(x, -20, 20)  # numerical stability\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y).astype(float)\n",
    "        m = X.shape[0]\n",
    "\n",
    "        eps = 1e-15\n",
    "        y_mean = np.clip(np.mean(y), eps, 1 - eps)\n",
    "        self.init_pred = np.log(y_mean / (1 - y_mean))\n",
    "        pred = np.full(m, self.init_pred, dtype=float)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            prob = self.sigmoid(pred)\n",
    "            grad = prob - y\n",
    "            hess = prob * (1 - prob)\n",
    "\n",
    "            stump = DecisionStumpXGB(\n",
    "                depth=1,\n",
    "                max_depth=self.max_depth,\n",
    "                reg_lambda=self.reg_lambda,\n",
    "                gamma=self.gamma,\n",
    "                subsample_features=self.subsample_features,\n",
    "                n_thresholds=self.n_thresholds\n",
    "            )\n",
    "            stump.fit(X, grad, hess)\n",
    "            update = stump.predict(X)\n",
    "            pred += self.learning_rate * update\n",
    "            self.trees.append(stump)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "        m = X.shape[0]\n",
    "        pred = np.full(m, self.init_pred, dtype=float)\n",
    "        for stump in self.trees:\n",
    "            pred += self.learning_rate * stump.predict(X)\n",
    "        return self.sigmoid(pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= self.proba_threshold).astype(int)\n",
    "\n",
    "# Multiclass XGBoost Classifier\n",
    "\n",
    "class XGBoostMulticlass:\n",
    "\n",
    "    def __init__(self, num_classes=10, **kwargs):\n",
    "        self.num_classes = num_classes\n",
    "        self.models = [XGBoostClassifier(**kwargs) for _ in range(num_classes)]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        for k in range(self.num_classes):\n",
    "            print(f\"Training class {k} vs rest...\")\n",
    "            y_binary = (y == k).astype(int)\n",
    "            self.models[k].fit(X, y_binary)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "        # Get raw logits from each binary classifier\n",
    "        logits = np.column_stack([self._model_logits(model, X) for model in self.models])\n",
    "        # Softmax for calibrated probabilities\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # stability trick\n",
    "        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "    def _model_logits(self, model, X):\n",
    "        m = X.shape[0]\n",
    "        pred = np.full(m, model.init_pred, dtype=float)\n",
    "        for tree in model.trees:\n",
    "            pred += model.learning_rate * tree.predict(X)\n",
    "        return pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca159c",
   "metadata": {},
   "source": [
    "KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd8053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=5, batch_size=500):\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _normalize(self, X):\n",
    "        # Scale to 0–1\n",
    "        X = X.astype(np.float32) / 255.0\n",
    "        # L2 normalize each sample (very important for cosine distance)\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-6\n",
    "        return X / norms\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = self._normalize(X)\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self._normalize(X)\n",
    "        n_test = X.shape[0]\n",
    "        predictions = np.zeros(n_test, dtype=int)\n",
    "\n",
    "        for i in range(0, n_test, self.batch_size):\n",
    "            X_batch = X[i:i+self.batch_size]\n",
    "\n",
    "            # Cosine similarity = dot(x, y)\n",
    "            sim = np.dot(X_batch, self.X_train.T)\n",
    "\n",
    "            # Convert similarity to distance\n",
    "            dists = 1 - sim   # smaller = closer\n",
    "\n",
    "            # Find top-k nearest neighbors\n",
    "            k_idx = np.argpartition(dists, self.k, axis=1)[:, :self.k]\n",
    "\n",
    "            k_labels = self.y_train[k_idx]\n",
    "            k_dists = dists[np.arange(dists.shape[0])[:, None], k_idx]\n",
    "\n",
    "            # Weight = 1 / distance (closer neighbor gets more power)\n",
    "            weights = 1 / (k_dists + 1e-6)\n",
    "\n",
    "            # Weighted vote\n",
    "            batch_pred = [\n",
    "                np.bincount(k_labels[row], weights=weights[row]).argmax()\n",
    "                for row in range(k_labels.shape[0])\n",
    "            ]\n",
    "\n",
    "            predictions[i:i+self.batch_size] = batch_pred\n",
    "\n",
    "        return predictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
